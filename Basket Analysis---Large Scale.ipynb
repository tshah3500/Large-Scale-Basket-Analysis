{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import random\n",
    "from scipy.sparse import csr_matrix, csc_matrix, coo_matrix\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "from mlxtend.frequent_patterns import association_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all the data---should take around a 1.5 minutes. Files names have been renamed to generics for confidentiality reasons.\n",
    "xls = pandas.ExcelFile('Data.xlsx')\n",
    "\n",
    "df1 = pandas.read_excel(xls, '8.8.2023-10.7.2023')\n",
    "df2 = pandas.read_excel(xls, '10.8.2023-12.7.2023')\n",
    "df3 = pandas.read_excel(xls, '12.8.2023-1.7.2024')\n",
    "df4 = pandas.read_excel(xls, '1.8.2024-3.7.2024')\n",
    "df5 = pandas.read_excel(xls, '3.8.2024-5.7.2024')\n",
    "df6 = pandas.read_excel(xls, '5.8.2024-8.7.2024')\n",
    "\n",
    "pt_data = pandas.concat([df1, df2, df3, df4, df5, df6], ignore_index=True)[['Order #', 'item_number', 'tran_qty']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Items: 207570\n",
      "Number of Transactions: 2881211\n",
      "Number of Transaction Lines: 5272937\n"
     ]
    }
   ],
   "source": [
    "#Store key values of the orders\n",
    "number_of_items = pt_data['item_number'].unique().size\n",
    "number_of_transactions = pt_data['Order #'].unique().size\n",
    "number_of_transaction_lines = pt_data['Order #'].size\n",
    "\n",
    "print(\"Number of Items: \" + str(number_of_items))\n",
    "print(\"Number of Transactions: \" + str(number_of_transactions))\n",
    "print(\"Number of Transaction Lines: \" + str(number_of_transaction_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts all Item IDs and Transaction IDs to integer values and creates a key\n",
    "#This step is nessecary because Sparse Matrices can only proccess numerics\n",
    "\n",
    "item_ids = pt_data['item_number'].unique().tolist()\n",
    "transaction_ids = pt_data['Order #'].unique().tolist()\n",
    "\n",
    "def generate_unique_id(existing_ids, limit):\n",
    "    while True:\n",
    "        new_id = random.randint(0, limit-1)\n",
    "        if new_id not in existing_ids:\n",
    "            existing_ids.add(new_id)\n",
    "            return new_id\n",
    "        \n",
    "def format_qty(quantity):\n",
    "    if quantity >= 1:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "existing_item_ids = set()\n",
    "existing_transaction_ids = set()\n",
    "\n",
    "\n",
    "item_ids_key = {x: generate_unique_id(existing_item_ids, number_of_items) for x in item_ids}\n",
    "transaction_ids_key = {x: generate_unique_id(existing_transaction_ids, number_of_transactions) for x in transaction_ids}\n",
    "\n",
    "pt_data['item_number'] = pt_data['item_number'].apply(lambda x: item_ids_key[x])\n",
    "pt_data['Order #'] = pt_data['Order #'].apply(lambda x: transaction_ids_key[x])\n",
    "pt_data['tran_qty'] = pt_data['tran_qty'].apply(format_qty)\n",
    "\n",
    "pt_data.rename(columns={\"Order #\": \"order\", \"tran_qty\": \"qty\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conversion to Sparse Matrix Format\n",
    "sparse_matrix = csr_matrix((pt_data.qty, (pt_data.order, pt_data.item_number)), shape=(number_of_transactions, number_of_items))\n",
    "sparse_matrix_coo = sparse_matrix.tocoo()\n",
    "mask = numpy.isnan(sparse_matrix_coo.data)\n",
    "sparse_matrix_coo.data[mask] = 0 \n",
    "sparse_matrix_coo.data = numpy.where(sparse_matrix_coo.data > 0, 1, 0)\n",
    "sparse_matrix = sparse_matrix_coo.tocsr()\n",
    "df = pandas.DataFrame.sparse.from_spmatrix(sparse_matrix)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tarunshah/Desktop/Venv VS/.venv/lib/python3.12/site-packages/mlxtend/frequent_patterns/fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Basket Analysis Calulations---keep the min_support value low because of the very high transaction volume.\n",
    "frequent_itemsets = fpgrowth(df, min_support=0.0001, use_colnames=True)\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.5)\n",
    "rules = rules.sort_values(by='confidence', ascending=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Integer IDs back into the original item IDs\n",
    "swapped_items_key = {v: k for k, v in item_ids_key.items()}\n",
    "\n",
    "def translate_ids(x):\n",
    "    ids = []\n",
    "    for val in x:\n",
    "        try:\n",
    "            ids.append(swapped_items_key[int(val)])\n",
    "        except:\n",
    "            ids.append('Unlocatable ID')    \n",
    "    return(ids)\n",
    "\n",
    "rules['antecedents'] = rules['antecedents'].apply(translate_ids)\n",
    "rules['consequents'] = rules['consequents'].apply(translate_ids)\n",
    "rules['Transactions Per Year'] = rules['support'].apply(lambda x: int(x*number_of_transactions)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prune redudant baskets---for large baskets, the algorithm will also identify every possible permutation of smaller baskets\n",
    "#This can make your final dataset explode from 10,000 useful baskets to millions of redudant baskets. Pruning removes these redundancies\n",
    "rules['itemset'] = rules.apply(lambda row: frozenset(row['antecedents']).union(row['consequents']), axis=1)\n",
    "rules = rules.sort_values(by='itemset', key=lambda x: x.str.len(), ascending=False)\n",
    "\n",
    "seen_itemsets = set()\n",
    "indices_to_keep = []\n",
    "\n",
    "for index, row in rules.iterrows():\n",
    "    itemset = row['itemset']\n",
    "    if not any(itemset < seen_itemset for seen_itemset in seen_itemsets):\n",
    "        indices_to_keep.append(index)\n",
    "        seen_itemsets.add(itemset)\n",
    "\n",
    "exportable_dataset = rules.loc[indices_to_keep]\n",
    "exportable_dataset.reset_index(drop=True, inplace=True)\n",
    "exportable_dataset.drop(columns=['itemset'], inplace=True)\n",
    "exportable_dataset = exportable_dataset[['antecedents', 'consequents', 'support', 'confidence', 'lift', 'Transactions Per Year']]\n",
    "exportable_dataset.to_csv('PartsTown Year-Long Analysis---Pruned Redundancy Dataset.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
